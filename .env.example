# ============================================
# BLENDER AI AUTOMATION - ENVIRONMENT CONFIG
# ============================================

# Copy this file to .env and fill in your actual values
# Command: cp .env.example .env


# ============================================
# AI PROVIDER SELECTION
# ============================================

# Choose your AI provider: 'local', 'claude', or 'openai'
# - 'local': Use local LLM (Ollama, LM Studio, etc.) - FREE, runs on your machine
# - 'claude': Use Anthropic Claude API - Paid, best quality
# - 'openai': Use OpenAI GPT API - Paid, good quality
AI_PROVIDER=local


# ============================================
# LOCAL LLM CONFIGURATION (Ollama, LM Studio, etc.)
# ============================================

# URL of your local LLM server
# Default for Ollama: http://localhost:11434/api/generate
# Default for LM Studio: http://localhost:1234/v1/chat/completions
LOCAL_LLM_URL=http://localhost:11434/api/generate

# Model name to use (must be installed on your local LLM server)
# Recommended models for code generation:
# - codellama (Meta's Code Llama - good for Python)
# - deepseek-coder (DeepSeek Coder - excellent for code)
# - mistral (Mistral - general purpose, fast)
# - llama3 (Meta Llama 3 - powerful general model)
# - qwen2.5-coder (Qwen Coder - very good for code)
LOCAL_LLM_MODEL=codellama

# Temperature for local LLM (0.0 = deterministic, 1.0 = creative)
# Lower is better for code generation (0.3-0.5 recommended)
LOCAL_LLM_TEMPERATURE=0.4

# Max tokens for local LLM response
LOCAL_LLM_MAX_TOKENS=4000


# ============================================
# ANTHROPIC CLAUDE API (if using AI_PROVIDER=claude)
# ============================================

# Get your API key from: https://console.anthropic.com/
# Leave empty if using local LLM
ANTHROPIC_API_KEY=

# Claude model to use (if using Claude)
# Options: claude-sonnet-4-20250514, claude-opus-4-5-20251101, claude-haiku-4-5-20251001
# Recommended: claude-sonnet-4-20250514 (best balance of speed/quality)
CLAUDE_MODEL=claude-sonnet-4-20250514


# ============================================
# OPENAI API (if using AI_PROVIDER=openai)
# ============================================

# Get your API key from: https://platform.openai.com/
# Leave empty if using local LLM
OPENAI_API_KEY=

# OpenAI model to use (if using OpenAI)
# Options: gpt-4-turbo-preview, gpt-4, gpt-3.5-turbo
# Recommended: gpt-4-turbo-preview (best for code)
OPENAI_MODEL=gpt-4-turbo-preview


# ============================================
# BLENDER CONFIGURATION
# ============================================

# REQUIRED: Path to your Blender executable
# Find this by running: which blender (Linux/Mac) or where blender (Windows)

# Linux example:
BLENDER_PATH=/usr/bin/blender

# Windows example (uncomment and modify):
# BLENDER_PATH=C:\Program Files\Blender Foundation\Blender 4.0\blender.exe

# macOS example (uncomment and modify):
# BLENDER_PATH=/Applications/Blender.app/Contents/MacOS/Blender

# If blender is in your PATH, you can just use:
# BLENDER_PATH=blender


# ============================================
# APPLICATION SETTINGS
# ============================================

# Default execution mode
# - 'background': Run Blender without GUI (faster, headless)
# - 'gui': Open Blender window to see the result
DEFAULT_MODE=background

# Temperature for AI generation (0.0 = deterministic, 1.0 = creative)
# For code: 0.3-0.5 recommended (more deterministic)
# For creative content: 0.7-0.9 (more varied)
TEMPERATURE=0.4

# Maximum tokens to generate (higher = longer code, but slower)
MAX_TOKENS=4000


# ============================================
# OUTPUT SETTINGS
# ============================================

# Automatically render the scene after generation
# true = always render, false = only render when --render flag used
AUTO_RENDER=false

# Automatically save .blend file after generation
# true = always save, false = only save when --save flag used
AUTO_SAVE=true

# Automatically export model after generation
# true = always export, false = only export when --export flag used
AUTO_EXPORT=false

# Export format (if AUTO_EXPORT=true or --export flag used)
# Options: obj, fbx, gltf, stl, ply
EXPORT_FORMAT=obj


# ============================================
# RENDER SETTINGS
# ============================================

# Render resolution (width x height in pixels)
RENDER_WIDTH=1920
RENDER_HEIGHT=1080

# Render samples (higher = better quality but slower)
# 32 = fast preview, 128 = good quality, 512+ = high quality
RENDER_SAMPLES=128

# Render engine
# Options: CYCLES (realistic, slower), EEVEE (fast, less realistic)
RENDER_ENGINE=CYCLES


# ============================================
# CODE VALIDATION & SAFETY
# ============================================

# Validate generated code before execution
# true = check for errors and security issues
# false = skip validation (faster but riskier)
VALIDATE_CODE=true

# Save generated code even if execution fails
# true = keep failed code for debugging
# false = only save successful generations
SAVE_FAILED_CODE=true

# Archive old generations
# true = keep history of all generations
# false = only keep latest
ARCHIVE_GENERATIONS=true

# Maximum number of regeneration attempts on failure
# If code fails validation, try regenerating this many times
MAX_RETRIES=3


# ============================================
# LOGGING SETTINGS
# ============================================

# Log level
# Options: DEBUG (verbose), INFO (normal), WARNING (errors only), ERROR, CRITICAL
LOG_LEVEL=INFO

# Save logs to file
# true = write to logs/blender_ai.log
# false = only console output
LOG_TO_FILE=true

# Log file path (relative to project root)
LOG_FILE=logs/blender_ai.log


# ============================================
# ADVANCED LOCAL LLM SETTINGS (Optional)
# ============================================

# Context window size (for local LLMs that support it)
# Larger = more context, but uses more memory
LOCAL_LLM_CONTEXT_SIZE=8192

# Number of GPU layers to offload (for faster inference)
# 0 = CPU only, -1 = all layers to GPU
# Requires compatible GPU and model
LOCAL_LLM_GPU_LAYERS=-1

# Batch size for local LLM
LOCAL_LLM_BATCH_SIZE=512

# Use mmap for faster model loading (true/false)
LOCAL_LLM_USE_MMAP=true

# Number of threads for CPU inference
# 0 = auto-detect, or set specific number (e.g., 8)
LOCAL_LLM_NUM_THREADS=0


# ============================================
# ALTERNATIVE LOCAL LLM PROVIDERS
# ============================================

# LM Studio Configuration (uncomment if using LM Studio)
# LOCAL_LLM_URL=http://localhost:1234/v1/chat/completions
# LOCAL_LLM_API_TYPE=openai  # LM Studio uses OpenAI-compatible API
# LOCAL_LLM_MODEL=local-model  # Model name in LM Studio

# Text Generation WebUI (Oobabooga) Configuration
# LOCAL_LLM_URL=http://localhost:5000/api/v1/generate
# LOCAL_LLM_API_TYPE=textgen

# LocalAI Configuration
# LOCAL_LLM_URL=http://localhost:8080/v1/chat/completions
# LOCAL_LLM_API_TYPE=openai


# ============================================
# PERFORMANCE TUNING
# ============================================

# Timeout for Blender execution (in seconds)
# Increase if working with complex scenes
BLENDER_TIMEOUT=300

# Enable GPU acceleration in Blender (if available)
# true = use GPU for rendering (faster)
# false = use CPU only
USE_GPU_RENDERING=true

# GPU device type (if USE_GPU_RENDERING=true)
# Options: CUDA (NVIDIA), OPTIX (NVIDIA RTX), METAL (Mac), HIP (AMD), ONEAPI (Intel)
GPU_DEVICE_TYPE=CUDA


# ============================================
# NOTES FOR LOCAL LLM USERS
# ============================================

# INSTALLING OLLAMA (Recommended for beginners):
# 1. Install Ollama: https://ollama.ai/
# 2. Pull a code model: ollama pull codellama
# 3. Start Ollama: ollama serve
# 4. Set AI_PROVIDER=local and LOCAL_LLM_MODEL=codellama above

# RECOMMENDED LOCAL MODELS FOR BLENDER CODE:
# - codellama:13b (Best balance of quality/speed)
# - deepseek-coder:6.7b (Excellent for code, faster)
# - qwen2.5-coder:7b (Very good, recent model)
# - mistral:7b (Fast, general purpose)

# INSTALLING MODELS IN OLLAMA:
# ollama pull codellama
# ollama pull deepseek-coder
# ollama pull qwen2.5-coder

# LM STUDIO SETUP:
# 1. Download LM Studio: https://lmstudio.ai/
# 2. Download a code model (search for "code" or "coder")
# 3. Start local server in LM Studio
# 4. Update LOCAL_LLM_URL above to LM Studio endpoint


# ============================================
# API COST COMPARISON (as of 2024)
# ============================================

# LOCAL LLM: FREE (uses your hardware)
# - One-time cost: GPU/good CPU
# - Running cost: Electricity only
# - Speed: Depends on hardware

# CLAUDE API: ~$3-15 per million tokens
# - Best quality for complex tasks
# - Fast inference
# - Pay per use

# OPENAI API: ~$2-10 per million tokens  
# - Good quality
# - Fast inference
# - Pay per use

# For experimentation: Start with LOCAL LLM (free)
# For production: Consider Claude or OpenAI APIs


# ============================================
# EXAMPLE CONFIGURATIONS
# ============================================

# Configuration 1: Local LLM with Ollama (FREE)
# AI_PROVIDER=local
# LOCAL_LLM_URL=http://localhost:11434/api/generate
# LOCAL_LLM_MODEL=codellama

# Configuration 2: Claude API (Best Quality)
# AI_PROVIDER=claude
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxx
# CLAUDE_MODEL=claude-sonnet-4-20250514

# Configuration 3: OpenAI API
# AI_PROVIDER=openai
# OPENAI_API_KEY=sk-xxxxxxxxxxxxx
# OPENAI_MODEL=gpt-4-turbo-preview

# Configuration 4: LM Studio (Local GUI)
# AI_PROVIDER=local
# LOCAL_LLM_URL=http://localhost:1234/v1/chat/completions
# LOCAL_LLM_MODEL=local-model
